





# Importing the required libraries : - 
import numpy as np 
import pandas as pd 
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import scipy.stats
import matplotlib.pyplot as plt
import plotly.express as px
import math
import warnings
warnings.filterwarnings('ignore')


df = pd.read_csv("C:\\Users\\salman_mohammed\\Desktop\\DATA mining\\loan_data_2017.csv")


# Set display options to show all columns
pd.set_option('display.max_columns', None)








df.shape


df.head()


df.nunique()


df.tail(5)


print(df.keys())








# Dropping the columns which has more than 2000  missing values in it
threshold = 1000
df= df.dropna(axis=1, thresh=df.shape[0] - threshold)


df.shape


print(df.keys())


df


df.isnull().sum()


# As we have missing values we can drop those rows which has NAN or null values in it
df = df.dropna()
pd.isnull(df).sum()





# Amount of data left after dropping the missing values
df.shape


df


#Removing Duplicate rows (if any)
counter = 0
rs,cs = df.shape

df.drop_duplicates(inplace=True)

if df.shape==(rs,cs):
    print('\n\033[1mInference:\033[0m The dataset doesn\'t have any duplicates')
else:
    print(f'\n\033[1mInference:\033[0m Number of duplicates dropped/fixed ---> {rs-df.shape[0]}')


df.describe()


df.nunique().sort_values()








# Columns to keep
desired_columns = ['loan_amnt','funded_amnt', 'term','int_rate','installment','emp_length_new','grade','home_ownership',
                   'annual_inc','verification_status','dti','delinq_2yrs','earliest_cr_line','inq_last_6mths','open_acc','pub_rec',
                   'loan_status','issue_d','total_pymnt','total_rec_int','revol_bal',
                   'revol_util','total_acc','initial_list_status','tax_liens']

columns_before_drop = df.columns # Store the columns before dropping

df.drop(df.columns.difference(desired_columns), axis=1, inplace=True)


# Identify the dropped columns
dropped_columns = set(columns_before_drop) - set(df.columns)

# Print the names of dropped columns
print("Dropped columns:", dropped_columns)


# The shape of dataset after dropping irrelevent columns

df.shape


#finding out the numerical and the categorical columns in dataset
features = [i for i in df.columns]
nu = df[features].nunique().sort_values()
nf = []; cf = []; nnf = 0; ncf = 0; #numerical & categorical features

for i in range(df[features].shape[1]):
    if nu.values[i]<=16:cf.append(nu.index[i])
    else: nf.append(nu.index[i])

print('\n\033[1mInference:\033[0m The Dataset has {} numerical & {} categorical features.'.format(len(nf),len(cf)))


df.head()











# Convert the date column to a Pandas datetime object

df['issue_d'] = pd.to_datetime(df['issue_d'], format='%b-%y')
df['issue_month'] = df['issue_d'].dt.month
df['issue_year'] = df['issue_d'].dt.year








# Convert the date column to a Pandas datetime object and overwrite the original column
df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], format='%b-%y')
df['earliest_cr_line_month'] = df['earliest_cr_line'].dt.month
df['earliest_cr_line_year'] = df['earliest_cr_line'].dt.year
df





# Remove unnecessary symbols and keep only numbers
df['term'] = df['term'].replace('[months]', '', regex=True)

# Convert the column to numeric type
df['term'] = pd.to_numeric(df['term'], errors='coerce')

# Display the modified DataFrame
print(df['term'].unique())





# examine int_rate
# Remove unnecessary symbols and keep only numbers
df['int_rate'] = df['int_rate'].replace('[%]', '', regex=True)

# Convert the column to numeric type
df['int_rate'] = pd.to_numeric(df['int_rate'], errors='coerce')

# Display the modified DataFrame
print(df['int_rate'].unique())















df.head(2)


columns = df[["loan_amnt", "funded_amnt", "int_rate", "installment", "dti", "delinq_2yrs", 
             "open_acc",  "revol_bal", "total_acc", 
             "total_pymnt", "total_rec_int", "tax_liens"]]
columns.describe()


from scipy.stats import skew

columns = df[["loan_amnt", "funded_amnt", "int_rate", "installment", "dti", "delinq_2yrs", 
             "open_acc",  "revol_bal", "total_acc", 
             "total_pymnt", "total_rec_int", "tax_liens"]]
# Create subplots
fig, axs = plt.subplots(nrows=4, ncols=3, figsize=(15, 15))
fig.subplots_adjust(hspace = 0.7, wspace = 0.4)  # Adjust the vertical space between subplots

# Plot histograms and add skewness score
for i, column in enumerate(columns):
    row, col = divmod(i, 3)
    axs[row, col].hist(df[column], bins=30, color='blue', alpha=0.7)
    axs[row, col].set_title(f'Histogram of {column} (Skewness: {round(skew(df[column]), 2)})')
    axs[row, col].set_xlabel(column)
    axs[row, col].set_ylabel('Frequency')
    axs[row, col].grid(True)

# Show the plot
plt.show()











value_counts = df['home_ownership'].value_counts()

# Create a bar plot
ax = value_counts.plot(kind='bar', color='skyblue')

# Add frequency counts on top of each bar
for p in ax.patches:
    ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 10), textcoords='offset points')

plt.title('Distribution of Home Ownership')
plt.xlabel('Home Ownership')
plt.ylabel('Count')
plt.show()









grade_counts = df['grade'].value_counts()

# Create a bar plot
ax = grade_counts.plot(kind='bar', color=['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#c2c2f0', '#ffb3e6', '#c2f0c2'])

# Add frequency counts on top of each bar
for p in ax.patches:
    ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 10), textcoords='offset points')

plt.title('Distribution of Loan Grades')
plt.xlabel('Grade')
plt.ylabel('Count')
plt.show()









verification_counts = df['verification_status'].value_counts()

# Create a pie chart
plt.pie(verification_counts, labels=verification_counts.index, autopct='%1.1f%%', startangle=90, colors=['#FF9999', '#66B2FF', '#99FF99'])
plt.title('Distribution of Verification Status')
plt.show()








loan_status_counts = df['loan_status'].value_counts()

# Create a bar plot
ax = loan_status_counts.plot(kind='bar', color=['#FF9999', '#66B2FF', '#99FF99'])

# Add frequency counts on top of each bar
for p in ax.patches:
    ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 10), textcoords='offset points')

plt.title('Distribution of Loan Status')
plt.xlabel('Loan Status')
plt.ylabel('Count')
plt.show()








term_counts = df['term'].value_counts()

# Create a bar plot
ax = term_counts.plot(kind='bar', color=['#FF9999', '#66B2FF'])

# Add frequency counts on top of each bar
for p in ax.patches:
    ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 10), textcoords='offset points')

plt.title('Distribution of Loan Terms')
plt.xlabel('Term (months)')
plt.ylabel('Count')
plt.show()











plt.figure(figsize=(10, 6))

# Grouped bar plot
sns.countplot(x='term', hue='loan_status', data=df, palette='viridis')

plt.title('Distribution of Loan Status Across Loan Terms')
plt.xlabel('Term (months)')
plt.ylabel('Count')
plt.legend(title='Loan Status')
plt.show()








plt.figure(figsize=(12, 8))

# Grouped bar plot
sns.countplot(x='home_ownership', hue='loan_status', data=df, palette='pastel')

plt.title('Distribution of Loan Status Across Homeownership')
plt.xlabel('Homeownership')
plt.ylabel('Count')
plt.legend(title='Loan Status')
plt.show()








plt.figure(figsize=(10, 6))

# Grouped bar plot
sns.countplot(x='grade', hue='loan_status', data=df, palette='coolwarm')

plt.title('Loan Status Across Grades')
plt.xlabel('Grade')
plt.ylabel('Count')
plt.legend(title='Loan Status')
plt.show()









plt.figure(figsize=(12, 8))
# Box plot
sns.boxplot(x='home_ownership', y='loan_amnt', data=df, palette='Set3')

plt.title('Loan Amount Across Homeownership')
plt.xlabel('Homeownership')
plt.ylabel('Loan Amount')
plt.show()









plt.figure(figsize=(12, 8))

# Box plot
sns.boxplot(x='term', y='loan_amnt', data=df, palette='viridis')

plt.title('Loan Amount Across Loan Terms')
plt.xlabel('Term (months)')
plt.ylabel('Loan Amount')
plt.show()





plt.figure(figsize=(12, 8))

# Hexbin plot
plt.hexbin(df['loan_amnt'], df['int_rate'], gridsize=30, cmap='Blues')

plt.title('Loan Amount vs. Interest Rate')
plt.xlabel('Loan Amount')
plt.ylabel('Interest Rate')
plt.colorbar(label='Count')
plt.show()








subset_df = df.sample(frac=0.1, random_state=42)  # Use a fraction that suits your needs

plt.figure(figsize=(12, 8))

# Box plot
sns.boxplot(x='home_ownership', y='annual_inc', data=subset_df, palette='Set2')

plt.title('Annual Income Across Homeownership (Subset)')
plt.xlabel('Homeownership')
plt.ylabel('Annual Income')
plt.show()








import matplotlib.pyplot as plt
import seaborn as sns

# Assuming df is your DataFrame
plt.figure(figsize=(12, 8))

# Box plot
sns.boxplot(x='grade', y='loan_amnt', data=df, order=['A', 'B', 'C', 'D', 'E', 'F', 'G'], palette='coolwarm')

plt.title('Loan Amount Across Grades')
plt.xlabel('Grade')
plt.ylabel('Loan Amount')
plt.show()









plt.figure(figsize=(12, 8))

# Scatter plot
plt.scatter(df['loan_amnt'], df['annual_inc'], alpha=0.5, color='blue')

plt.title('Loan Amount vs. Annual Income')
plt.xlabel('Annual Income')
plt.ylabel('Loan Amount')
plt.show()











import pandas as pd

# Create a binary target variable
df['loan_default'] = df['loan_status'].apply(lambda x: 1 if x not in ['Fully Paid', 'Current'] else 0)


df['loan_default'].value_counts()





#As the data from 'loan_status' is imputed to 'loan_default', we wil drop 'loan_status'
df.drop('loan_status', axis=1, inplace=True)



df


df.head()





# examine revol_util
# Remove unnecessary symbols and keep only numbers
df['revol_util'] = df['revol_util'].replace('[%]', '', regex=True)

# Convert the column to numeric type
df['revol_util'] = pd.to_numeric(df['revol_util'], errors='coerce')

# Display the modified DataFrame
print(df['revol_util'].unique())






df['grade'].value_counts()


grade_mapping = {
    'A': 0,
    'B': 1,
    'C': 2,
    'D': 3,
    'E': 4,
    'F': 5,
    'G': 6,
}

# Map 'grade' values to numeric values
df['grade'] = df['grade'].map(grade_mapping)





df['home_ownership'].value_counts()


df['loan_default'].value_counts()


home_ownership_mapping = {
    'MORTGAGE': 0,
    'RENT': 1,
    'OWN': 2,
}

# Map 'grade' values to numeric values
df['home_ownership'] = df['home_ownership'].map(home_ownership_mapping)





# Combine 'Source Verified' and 'Verified' into a single category
df['verification_status'] = df['verification_status'].replace({'Source Verified': 'Verified'})


df['verification_status'].value_counts()


verification_status_mapping = {
    'Not Verified': 0,
    'Verified': 1,
}

# Map 'grade' values to numeric values
df['verification_status'] = df['verification_status'].map(verification_status_mapping)


df.head()





df['initial_list_status'].value_counts()


initial_list_status_mapping = {
    'w': 0,
    'f': 1,
}

# Map 'grade' values to numeric values
df['initial_list_status'] = df['initial_list_status'].map(initial_list_status_mapping)


df.head()








import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Ensure that the target column is correctly named in your DataFrame
target_column = 'loan_default'

# Check if the target column exists in the DataFrame
if target_column not in df.columns:
    raise KeyError(f"The specified target column '{target_column}' is not present in the DataFrame.")

# Select relevant features
selected_features = [ 'loan_amnt', 'funded_amnt', 'term', 'int_rate', 'installment','grade', 'home_ownership',
                     'annual_inc', 'verification_status', 'dti', 'delinq_2yrs', 'earliest_cr_line', 'inq_last_6mths',
                     'open_acc', 'pub_rec', 'issue_d', 'total_pymnt', 'total_rec_int', 'revol_bal', 'revol_util',
                     'total_acc', 'initial_list_status', 'tax_liens']
       

# Subset the DataFrame with selected features and target variable
subset_df = df[selected_features + [target_column]]

# Encode categorical variables if needed (e.g., one-hot encoding)
subset_df = pd.get_dummies(subset_df, columns=['grade', 'home_ownership', 'verification_status', 'initial_list_status'], drop_first=True)

# Split the data into features (X) and target variable (y)
X = subset_df.drop(target_column, axis=1)
y = subset_df[target_column]

# Calculate correlation with the target variable
correlation_with_target = subset_df.corr()[target_column].abs().sort_values(ascending=False)

# Print correlation with the target variable
print("Correlation with the target variable:")
print(correlation_with_target)






from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import pandas as pd


# Ensure that the target column is correctly named in your DataFrame
target_column = 'loan_default'

# Check if the target column exists in the DataFrame
if target_column not in df.columns:
    raise KeyError(f"The specified target column '{target_column}' is not present in the DataFrame.")

# Select relevant features
selected_features = ['loan_amnt', 'funded_amnt', 'term', 'int_rate', 'installment', 'grade', 'home_ownership',
                     'annual_inc', 'verification_status', 'dti', 'delinq_2yrs', 'inq_last_6mths',
                     'open_acc', 'pub_rec', 'total_pymnt', 'total_rec_int', 'revol_bal', 'revol_util',
                     'total_acc', 'initial_list_status', 'tax_liens']

# Subset the DataFrame with selected features and target variable
subset_df = df[selected_features + [target_column]]

# Encode categorical variables if needed (e.g., one-hot encoding)
subset_df = pd.get_dummies(subset_df, columns=['grade', 'home_ownership', 'verification_status', 'initial_list_status'], drop_first=True)

# Split the data into features (X) and target variable (y)
X = subset_df.drop(target_column, axis=1)
y = subset_df[target_column]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a linear regression model
lr_model = LinearRegression()

# Use RFE for feature selection
num_selected_features = 10  # Adjust as needed
rfe = RFE(estimator=lr_model, n_features_to_select=num_selected_features)
X_rfe = rfe.fit_transform(X_train, y_train)

# Get the selected features
selected_features_rfe = X.columns[rfe.support_]

# Print selected features using RFE
print(f"Selected features using RFE with Linear Regression ({num_selected_features} features):")
print(selected_features_rfe)



# Plot a bar plot
feature_importance_df = pd.DataFrame({'Feature': selected_features_rfe, 'Importance': rfe.estimator_.coef_})

# Sort the DataFrame by importance scores in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot a bar plot
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')
plt.title('Feature Importance from RFE with Linear Regression')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.show()


# Columns to keep
desired_columns = ['loan_amnt','term','int_rate','grade','verification_status','dti','inq_last_6mths',
                    'loan_status','issue_d','total_pymnt', 'loan_default', 'int_rate','grade_2', 'grade_3', 'grade_4',
                    'grade_5', 'grade_6', 'home_ownership_1', 'home_ownership_2',
                    'initial_list_status_1']

columns_before_drop = df.columns # Store the columns before dropping

df.drop(df.columns.difference(desired_columns), axis=1, inplace=True)


import seaborn as sns
import matplotlib.pyplot as plt

# Select features excluding the target variable
features = df.drop(target_column, axis=1)

# Calculate the correlation matrix
correlation_matrix = features.corr()

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", vmin=-1, vmax=1)
plt.title('Correlation Heatmap')
plt.show()



# Calculate the correlation of features with the target variable
correlation_with_target = df.drop(target_column, axis=1).corrwith(df[target_column])

# Create a bar plot
plt.figure(figsize=(6, 4))
correlation_with_target.sort_values().plot(kind='barh', color='skyblue')
plt.title('Correlation of Features with Target Variable')
plt.xlabel('Correlation Coefficient')
plt.ylabel('Feature')
plt.show()

# Display a correlation table
correlation_table = pd.DataFrame({'Correlation with Target': correlation_with_target})
print(correlation_table)






df


columns = ['loan_amnt', 'term','int_rate','grade','verification_status','dti','inq_last_6mths','issue_d','total_pymnt', 'loan_default']
summary_statistics = df[columns].describe()
summary_statistics











def find_outliers(df, columns):
    outlier_info = {}

    for column in columns:
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - (1.5 * IQR)
        upper_bound = Q3 + (1.5 * IQR)
        outliers = (df[column] < lower_bound) | (df[column] > upper_bound)
        outlier_count = outliers.sum()

        outlier_info[column] = {
            'outlier_count': outlier_count,
            'IQR': IQR,
            'Q1': Q1,
            'Q3': Q3,
            'lower_bound': lower_bound,
            'upper_bound': upper_bound
        }

    return outlier_info

# Specify the columns for which you want to find outliers
outlier_columns = ['loan_amnt','term','int_rate','grade','verification_status','dti','inq_last_6mths',
                    'issue_d','total_pymnt']

outlier_info = find_outliers(df, outlier_columns)

for column, info in outlier_info.items():
    print(f"Column: {column}")
    print(f"Count of outliers: {info['outlier_count']}")
    print(f"IQR: {info['IQR']}")
    print(f"Q1: {info['Q1']}")
    print(f"Q3: {info['Q3']}")
    print(f"Lower Bound: {info['lower_bound']}")
    print(f"Upper Bound: {info['upper_bound']}\n")







# List of loan-related columns to create box plots
loan_columns = [ 'loan_amnt','term','int_rate','grade','verification_status','dti','inq_last_6mths',
                    'issue_d','total_pymnt']

# Loop through the car-related columns and create a box plot for each one
for column in loan_columns:
    fig = px.box(df, y=column)
    fig.update_layout(
        autosize=False,
        width=800,
        height=600,
        title='Box plot of ' + column,
        xaxis_title=column,
        yaxis_title='Value'
    )

    # Show the plot
    fig.show()











# Transformation for 'loan_amnt'
df['loan_amnt'] = np.log1p(df['loan_amnt'])

# Transformation for 'int_rate'
df['int_rate'] = np.log1p(df['int_rate'])

# Transformation for 'grade' (one-hot encoding)
df = pd.get_dummies(df, columns=['grade'], drop_first=True)

# Transformation for 'dti'
df['dti'] = np.log1p(df['dti'])

# Transformation for 'inq_last_6mths'
df['inq_last_6mths'] = np.log1p(df['inq_last_6mths'])

# Transformation for 'issue_d' (extracting the year)
df['issue_year'] = pd.to_datetime(df['issue_d']).dt.year

# Transformation for 'total_pymnt'
df['total_pymnt'] = np.log1p(df['total_pymnt'])


df


# Columns to plot
columns_to_plot = ['loan_amnt', 'int_rate', 'dti', 'inq_last_6mths', 'total_pymnt']

# Create subplots
fig, axes = plt.subplots(nrows=len(columns_to_plot), ncols=1, figsize=(10, 14))

# Loop through columns and plot histograms
for i, column in enumerate(columns_to_plot):
    sns.histplot(df[column], kde=True, bins=20, color='blue', ax=axes[i])
    axes[i].set_title(f'Histogram of Transformed {column}', fontsize=12)
    axes[i].set_xlabel(f'Transformed {column}', fontsize=10)
    axes[i].set_ylabel('Frequency', fontsize=10)

plt.tight_layout()
plt.show()


# # Loop through the car-related columns and create a box plot for each one
# for column in loan_columns:
#     fig = px.box(df, y=column)
#     fig.update_layout(
#         autosize=False,
#         width=800,
#         height=600,
#         title='Box plot of ' + column,
#         xaxis_title=column,
#         yaxis_title='Value'
#     )

#     # Show the plot
#     fig.show()











import category_encoders as ce
from sklearn.model_selection import train_test_split

#the columns that we are using
x_columns = ['loan_amnt', 'term', 'int_rate', 'verification_status', 'issue_d','dti', 'inq_last_6mths', 'total_pymnt', 'grade_1', 'grade_2', 'grade_3', 'grade_4', 'grade_5', 'grade_6', 'issue_year']
target_column = 'loan_default'


# Select features (X) and target variable (y)
X = df[x_columns]
y = df[target_column]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)

# Target Encoding on Training Set for 'verification_status'
encoder_verification_status = ce.TargetEncoder(cols=['verification_status'])
X_train_encoded = encoder_verification_status.fit_transform(X_train, y_train)

# Apply the Same Encoding to Test Sets for 'verification_status'
X_test_encoded = encoder_verification_status.transform(X_test)

# Target Encoding on Training Set for 'grade_1'
encoder_grade_1 = ce.TargetEncoder(cols=['grade_1'])
X_train_encoded_grade_1 = encoder_grade_1.fit_transform(X_train_encoded, y_train)

# Apply the Same Encoding to Validation and Test Sets for 'grade_1'
X_test_encoded_grade_1 = encoder_grade_1.transform(X_test_encoded)



X_train = X_train_encoded_grade_1 
X_test = X_train_encoded_grade_1





df











print(df[df.isin([np.inf, -np.inf]).any(axis=1)]) 


print(df[df.isna().any(axis=1)])


# Drop rows with infinite values
df = df.replace([np.inf, -np.inf], np.nan).dropna()

#fill infinite values with a specific value
df.replace([np.inf, -np.inf], np.nan, inplace=True)

# fill NaN values with the median of each column
df.fillna(df.median(), inplace=True)









import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns


# Specifying features and target variable
features = ['loan_amnt', 'term', 'int_rate', 'verification_status', 'dti', 'inq_last_6mths', 'total_pymnt', 'grade_1', 'grade_2', 'grade_3', 'grade_4', 'grade_5', 'grade_6', 'issue_year']
target = 'loan_default'

# Select features (X) and target variable (y)
X = df[features]
y = df[target]

# Split the data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# Initialize the Logistic Regression model
logreg_model = LogisticRegression(random_state=42)

# Train the model on the training set
logreg_model.fit(X_train, y_train)

# Make predictions on the validation set
y_val_pred = logreg_model.predict(X_val)

# Evaluate the model on the validation set
accuracy_val = accuracy_score(y_val, y_val_pred)
conf_matrix_val = confusion_matrix(y_val, y_val_pred)
class_report_val = classification_report(y_val, y_val_pred)

# Print the evaluation metrics for the validation set
print(f"Validation Accuracy: {accuracy_val:.4f}")
print("\nValidation Confusion Matrix:")
print(conf_matrix_val)
print("\nValidation Classification Report:")
print(class_report_val)

# Make predictions on the test set
y_test_pred = logreg_model.predict(X_test)

# Evaluate the model on the test set
accuracy_test = accuracy_score(y_test, y_test_pred)
conf_matrix_test = confusion_matrix(y_test, y_test_pred)
class_report_test = classification_report(y_test, y_test_pred)

# Print the evaluation metrics for the test set
print(f"\nTest Accuracy: {accuracy_test:.4f}")
print("\nTest Confusion Matrix:")
print(conf_matrix_test)
print("\nTest Classification Report:")
print(class_report_test)


# Plot the confusion matrix using seaborn
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix_test, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=logreg_model.classes_, yticklabels=logreg_model.classes_)
plt.title('Confusion Matrix - Test Set')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()









# Specifying features and target variable
features = ['loan_amnt', 'term', 'int_rate', 'verification_status', 'dti', 'inq_last_6mths', 'total_pymnt', 'grade_1', 'grade_2', 'grade_3', 'grade_4', 'grade_5', 'grade_6', 'issue_year']
target = 'loan_default'

# Select features (X) and target variable (y)
X = df[features]
y = df[target]

# Split the data into training (60%) and temporary set (40%)
X_train_temp, X_temp, y_train_temp, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)

# Further split the temporary set into validation (50%) and test (50%)
X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

# Now, X_train, y_train, X_validation, y_validation, X_test, y_test are ready for training, validation, and testing

# Initialize the Decision Tree model
dt_model = DecisionTreeClassifier(random_state=42)

# Train the model on the training set
dt_model.fit(X_train_temp, y_train_temp)

# Make predictions on the validation set
y_pred_validation = dt_model.predict(X_validation)

# Evaluate the model on the validation set
accuracy_validation = accuracy_score(y_validation, y_pred_validation)
conf_matrix_validation = confusion_matrix(y_validation, y_pred_validation)
class_report_validation = classification_report(y_validation, y_pred_validation)

# Print the evaluation metrics for the validation set
print("Validation Set Metrics:")
print(f"Accuracy: {accuracy_validation:.4f}")
print("\nConfusion Matrix:")
print(conf_matrix_validation)
print("\nClassification Report:")
print(class_report_validation)

# Make predictions on the test set
y_pred_test = dt_model.predict(X_test)

# Evaluate the model on the test set
accuracy_test = accuracy_score(y_test, y_pred_test)
conf_matrix_test = confusion_matrix(y_test, y_pred_test)
class_report_test = classification_report(y_test, y_pred_test)

# Print the evaluation metrics for the test set
print("\nTest Set Metrics:")
print(f"Accuracy: {accuracy_test:.4f}")
print("\nConfusion Matrix:")
print(conf_matrix_test)
print("\nClassification Report:")
print(class_report_test)


# Plot the confusion matrix using seaborn
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix_test, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=dt_model.classes_, yticklabels=dt_model.classes_)
plt.title('Confusion Matrix - Test Set')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()








# Specifying features and target variable
features = ['loan_amnt', 'term', 'int_rate', 'verification_status', 'dti', 'inq_last_6mths', 'total_pymnt', 'grade_1', 'grade_2', 'grade_3', 'grade_4', 'grade_5', 'grade_6', 'issue_year']
target = 'loan_default'

# Select features (X) and target variable (y)
X = df[features]
y = df[target]

# Split the data into training (60%), validation (20%), and testing (20%) sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)
X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

# Initialize the Random Forest model
rf_model = RandomForestClassifier(random_state=42)

# Train the model on the training set
rf_model.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_validation = rf_model.predict(X_validation)

# Evaluate the model on the validation set
accuracy_validation = accuracy_score(y_validation, y_pred_validation)
conf_matrix_validation = confusion_matrix(y_validation, y_pred_validation)
class_report_validation = classification_report(y_validation, y_pred_validation)

# Print the evaluation metrics for the validation set
print("Validation Set Metrics:")
print(f"Accuracy: {accuracy_validation:.4f}")
print("\nConfusion Matrix:")
print(conf_matrix_validation)
print("\nClassification Report:")
print(class_report_validation)

# Make predictions on the test set
y_pred_test = rf_model.predict(X_test)

# Evaluate the model on the test set
accuracy_test = accuracy_score(y_test, y_pred_test)
conf_matrix_test = confusion_matrix(y_test, y_pred_test)
class_report_test = classification_report(y_test, y_pred_test)

# Print the evaluation metrics for the test set
print("\nTest Set Metrics:")
print(f"Accuracy: {accuracy_test:.4f}")
print("\nConfusion Matrix:")
print(conf_matrix_test)
print("\nClassification Report:")
print(class_report_test)


# Plot the confusion matrix using seaborn
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix_test, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=rf_model.classes_, yticklabels=rf_model.classes_)
plt.title('Confusion Matrix - Test Set')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()






# Specifying features and target variable
features = ['loan_amnt', 'term', 'int_rate', 'verification_status', 'dti', 'inq_last_6mths', 'total_pymnt', 'grade_1', 'grade_2', 'grade_3', 'grade_4', 'grade_5', 'grade_6', 'issue_year']
target = 'loan_default'

# Select features (X) and target variable (y)
X = df[features]
y = df[target]

# Split the data into training (60%), validation (20%), and test (20%) sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)
X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

# Initialize XGBoost model with the desired evaluation metric
model = xgb.XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',  # Specify the evaluation metric for training
    eta=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    seed=42
)

# Train the model on the training set
model.fit(
    X_train, y_train,
    eval_set=[(X_validation, y_validation)],
    early_stopping_rounds=10
)

# Make predictions on the test set
y_proba = model.predict_proba(X_test)[:, 1]  # Get the predicted probabilities for the positive class

# Convert probabilities to binary predictions using a threshold (e.g., 0.5)
y_pred = (y_proba > 0.5).astype(int)

# Evaluate the model on the test set
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Print the evaluation metrics for the test set
print("Test Set Metrics:")
print(f"Accuracy: {accuracy:.4f}")
print("\nConfusion Matrix:")
print(conf_matrix)
print("\nClassification Report:")
print(class_report)



conf_matrix = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix using seaborn
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=model.classes_, yticklabels=model.classes_)
plt.title('Confusion Matrix - Test Set')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()













